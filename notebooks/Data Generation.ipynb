{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation\n",
    "\n",
    "The purpose of this project is to demonstrate how to regularize models for performance and for fairness. Do do this, we need a method to generate data with bivariate correlations among a number of groups. This is doable with some fun statistics, adapted from the R and SPSS code presented by [David Howell at UVM](https://www.uvm.edu/~statdhtx/StatPages/More_Stuff/CorrGen.html)\n",
    "\n",
    "The basic steps are\n",
    "* generate random normal variables for X and y ($\\mu=0, \\sigma=1$)\n",
    "* declare a desired correlation coefficient between X and y, $\\rho$\n",
    "* Calculate $a = \\rho{\\sqrt(1-\\rho^2)}$\n",
    "* Calculate $Z = a*X + y$.\n",
    "* (optional) Adjust the means and variances of X and Z to what you want them to be by simple linear transformations--(e.g., $ \\hat{X} = X^{\\prime} * \\hat\\sigma + \\hat\\mu$).\n",
    "\n",
    "We then have to generalize this to a multivariate dataset, which involves the same process but with some matrix algebra, which actually streamlines the process considerably\n",
    "\n",
    "* create a covariance matrix with desired correlation between all variables (y and X) \n",
    "* generate a random multivariate normal distribution with this cov matrix\n",
    "* adjust means and variances of X and y to fit desired criteria (i.e., de-normalize the data if desired)\n",
    "\n",
    "## Creating Group Differences\n",
    "\n",
    "For both cases, the desired outcome is to separate groups within the generated data by a known amount. This is as simple as subsetting the data and adjusting the mean of each group through a linear combination (e.g., $X|g + b$, where b is the effect size of the bias). I've provided examples of three types of common effect sizes for assessing bias:\n",
    "\n",
    "* Cohen's $d$: standard difference of means between groups. Here, I use the multivariate extention of $d$, which is the standard difference between a single group and the population mean. For example, a data with differences between Male, Female and Non-binary groups might show Female $d = -.07$, indicating that the Female group is 7% of a standard deviation below the mean of all three groups. Effect size $d$ is most applicable for continuous predictions like regression. \n",
    "* proportional pass rates, also known as \"selection rates\": proportional pass rates of each group, independent of each other group. For example, a data with differences between Male, Female and Non-binary groups might show Female $prop = .47$, indicating that the Female group passes the decision threshold 47% of the time. See below for notes on decision thresholds. Most often, proportional pass rates are *dependent*, meaning that they are only indicative of impact relative to other groups. A Female selection rate of .40 is fair if the Male selection rate is .42 ($.40/.42 = .952$), but unfair if the Male selection rate is .60 ($.40/.60 = .667$). This proportion of proportions is known in legal terms as an **impact ratio**. Note that proportional effect size is only applicable to classification. \n",
    "* percentile pass rate: the same as proportional pass rates, but based on percentiles rather than proportions.\n",
    "\n",
    "## Continuous vs Binary Precitions\n",
    "Both univariate and multivariate instances will generate continuous X and y variables. Converting these variables to binary outcomes is done by boolian logic of $y > threshold$, where the threshold is set to the expected value of the  population mean, 0. \n",
    "\n",
    "It is important to note that generated data will *always* approach the corrlations indiated by the covariance matrix given a large enough sample. Binarization of a continuous variable will be proportional to the input covariance matrix, but **will always be** less correlated. This is because binarization of an underlying continuous variable will always increase variance. A better way of correlating binarized continuous variables with a continuous variable might be something like a partial biserial correlation. I'm going to keep that as a future idea for now, though.\n",
    "\n",
    "## This Notebook\n",
    "\n",
    "Here, I go through the code inline, and generate samples with different parameters and desired uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# this is for a weird tab completion error on my machine\n",
    "%config Completer.use_jedi = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate\n",
    "\n",
    "Sometimes you just want two variables to be correlated. This is a fun algebraic method, and pretty easy to set up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "np.random.seed(8675309)\n",
    "mu_hat_x = 0\n",
    "mu_hat_z = 10\n",
    "sigma_hat_x = 1\n",
    "sigma_hat_z = 10\n",
    "\n",
    "def generate_corr_data_univariate(rho: float, \n",
    "                                  N: int, \n",
    "                                  seed: int=None,\n",
    "                                  verbose: bool=True, \n",
    "                                  reshape_dict: dict=None) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Function for simulating univariate correlated data. Given an approximate\n",
    "    correlation coefficient, rho, simulate data of size N for two random Normal\n",
    "    variables. Can also include dictionary for reshaping output data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rho : float, range(-1,1)\n",
    "        value of target approximate correlation for output vectors\n",
    "        will have normal variance as a function of N\n",
    "    N : int, range(2,inf)\n",
    "        length of output\n",
    "    seed : int\n",
    "        optional argument to set random seed\n",
    "    verbose : bool\n",
    "        argument for printing steps\n",
    "    reshape_dict : dict[**vargs]\n",
    "        dictionary to reshape output with the following keys:\n",
    "        'mu_hat_x' : output mean of X\n",
    "        'sigma_hat_x' : output standard deviation of X\n",
    "        'mu_hat_z' : output mean of z\n",
    "        'sigma_hat_z' : output standard deviation of z        \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X, z : np.array\n",
    "        numpy arrays of size N, approximately correlated at rho\n",
    "    \n",
    "    \"\"\"\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    X = np.random.normal(0, 1, size=N)\n",
    "    y = np.random.normal(0, 1, size=N)\n",
    "\n",
    "    a = rho/np.sqrt(1-rho**2)\n",
    "    z = a*X+y\n",
    "\n",
    "    if verbose:\n",
    "        print(f'generating data with approxmate rho={rho}\\n')\n",
    "        print(f'seeded X mean: {X.mean(): .3f}, sd: {X.std(): .3f}')\n",
    "        print(f'seeded y mean: {y.mean(): .3f}, sd: {y.std(): .3f}')\n",
    "        print(f'simulated y (z) mean: {z.mean(): .3f}, sd: {z.std(): .3f}\\n')\n",
    "\n",
    "    # optional, if input moments differ from desired output moments\n",
    "    if reshape_dict:\n",
    "        rd = reshape_dict\n",
    "        X = X*rd['sigma_hat_x']+rd['mu_hat_x']\n",
    "        z = z*rd['sigma_hat_z']+rd['mu_hat_z']\n",
    "\n",
    "    if verbose:\n",
    "        print(f'adjusted X mean: {X.mean(): .3f}, sd: {X.std(): .3f}')\n",
    "        print(f'adjusted simulated y (z) mean: {z.mean(): .3f}, sd: {z.std(): .3f}\\n')\n",
    "        print(f'correlation of z~X: {pearsonr(X, z)[0]:.3f}')\n",
    "    \n",
    "    return z, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating data with approxmate rho=0.5\n",
      "\n",
      "seeded X mean:  0.019, sd:  0.979\n",
      "seeded y mean:  0.071, sd:  0.997\n",
      "simulated y (z) mean:  0.082, sd:  1.126\n",
      "\n",
      "adjusted X mean:  0.019, sd:  0.979\n",
      "adjusted simulated y (z) mean:  0.082, sd:  1.126\n",
      "\n",
      "correlation of z~X: 0.466\n"
     ]
    }
   ],
   "source": [
    "z, X = generate_corr_data_univariate(rho= .5, N = 1000, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating data with approxmate rho=0.7\n",
      "\n",
      "seeded X mean:  0.019, sd:  0.979\n",
      "seeded y mean:  0.071, sd:  0.997\n",
      "simulated y (z) mean:  0.090, sd:  1.355\n",
      "\n",
      "adjusted X mean:  0.019, sd:  0.979\n",
      "adjusted simulated y (z) mean:  0.090, sd:  1.355\n",
      "\n",
      "correlation of z~X: 0.678\n"
     ]
    }
   ],
   "source": [
    "z, X = generate_corr_data_univariate(rho= .7, N = 1000, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping data to have different means and standard deviations, while retaining the approximate correlation we initalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating data with approxmate rho=0.5\n",
      "\n",
      "seeded X mean:  0.016, sd:  0.973\n",
      "seeded y mean:  0.042, sd:  0.991\n",
      "simulated y (z) mean:  0.051, sd:  1.144\n",
      "\n",
      "adjusted X mean:  0.016, sd:  0.973\n",
      "adjusted simulated y (z) mean:  25.254, sd:  5.721\n",
      "\n",
      "correlation of z~X: 0.500\n"
     ]
    }
   ],
   "source": [
    "reshape_dict={'mu_hat_x' : 0,\n",
    "        'sigma_hat_x' : 1,\n",
    "        'mu_hat_z' : 25,\n",
    "        'sigma_hat_z' : 5}\n",
    "z, X = generate_corr_data_univariate(rho= .5, N = 1000, seed=1234, reshape_dict=reshape_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate\n",
    "\n",
    "Say that we want $k$ variables with differing correlations. We can't just repeate the univariate case $k$ times. Rather, we have fun with linearl algebra, and generate a multivariate normal distribution with set covariance between each of the $k$ variables.\n",
    "\n",
    "Note that not all covariance matrices are possible. All true covariance matrices are constrained to be [positive and semi-definite](https://stats.stackexchange.com/questions/52976/is-a-sample-covariance-matrix-always-symmetric-and-positive-definite), although you can certainly try to coerce an impossible covariance matrix. Scipy will tell you if you try something too weird :)\n",
    "\n",
    "I've also added some helper functions for generating covariance matrices of a given range. In the more robust `generate_group_data` function, I also include a string labeling convention for small, moderate and large covariances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "\n",
    "def generate_corr_data_multivariate(rmat: np.ndarray, \n",
    "                                    N: int, \n",
    "                                    seed: int=None,\n",
    "                                    verbose: bool=True, \n",
    "                                    colnames: list=None,\n",
    "                                    reshape_mu: np.array=None,\n",
    "                                    reshape_sigma: np.array=None,\n",
    "                                   ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Function for simulating univariate correlated data. Given an approximate\n",
    "    correlation coefficient, rho, simulate data of size N for two random Normal\n",
    "    variables. Can also include dictionary for reshaping output data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    rho : float, range(-1,1)\n",
    "        value of target approximate correlation for output vectors\n",
    "        will have normal variance as a function of N\n",
    "    N : int, range(2,inf)\n",
    "        length of output\n",
    "    seed : int\n",
    "        optional argument to set random seed\n",
    "    verbose : bool\n",
    "        argument for printing steps\n",
    "    reshape_mu : np.array\n",
    "        array to reshape output means\n",
    "    reshape_sigma : np.array\n",
    "        array to reshape output standard deviations\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output : pandas.DataFrame\n",
    "        df shape (N,len(rmat)), with columns approximately correlated at rho\n",
    "    \"\"\"\n",
    "    if seed:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    mu = np.zeros(len(rmat))\n",
    "    \n",
    "    mat = np.random.multivariate_normal(mu, cov=rmat,size=N)\n",
    "\n",
    "    # optional, if input moments differ from desired output moments\n",
    "    if reshape_sigma:\n",
    "        mat = mat * reshape_sigma\n",
    "    if reshape_mu:\n",
    "        mat = mat + reshape_mu\n",
    "        \n",
    "    output = pd.DataFrame(mat, columns=colnames)\n",
    "\n",
    "    if verbose:\n",
    "        print(f'target rho \\n{rmat}')\n",
    "        print(f'achieved rho \\n{pd.DataFrame(mat).corr()}')\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "def make_cov_array(n, var_range=(0,1)):\n",
    "    return np.random.uniform(var_range[0], var_range[1], len(list(combinations(range(n),2))))\n",
    "    \n",
    "\n",
    "def make_cov_matrix(n, cov_array):\n",
    "    \"\"\"\n",
    "    generate a symetric covariance matrix given an array of\n",
    "    correlations between columns, in combinatoric order of \n",
    "    $_nC_2$\n",
    "    \n",
    "    e.g., three element matrix\n",
    "    cov_array = cor(A,B), cor(A,C), cor(B,C)\n",
    "    \n",
    "    e.g., four element matrix\n",
    "    cov_array = cor(A,B), cor(A,C), cor(A,D), cor(B,C), cor(B,D), cor(C,D)\n",
    "    \"\"\"\n",
    "    M = np.ones([n,n])\n",
    "    M[np.triu_indices(M.shape[0], k = 1)] = cov_array\n",
    "    M.T[np.triu_indices(M.shape[0], k = 1)] = cov_array\n",
    "    return M\n",
    "\n",
    "def make_random_cov_matrix(n, var_range):\n",
    "    return make_cov_matrix(n, make_cov_array(n, var_range))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 0.1, 0.2],\n",
       "       [0.1, 1. , 0.3],\n",
       "       [0.2, 0.3, 1. ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_cov_matrix(3, cov_array=[.1, .2, .3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 0.1, 0.2, 0.3],\n",
       "       [0.1, 1. , 0.4, 0.5],\n",
       "       [0.2, 0.4, 1. , 0.6],\n",
       "       [0.3, 0.5, 0.6, 1. ]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_cov_matrix(4, cov_array=[.1, .2, .3, .4, .5, .6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.64739051, 0.39180595, 0.97991933],\n",
       "       [0.64739051, 1.        , 0.44433563, 0.80767941],\n",
       "       [0.39180595, 0.44433563, 1.        , 0.96963645],\n",
       "       [0.97991933, 0.80767941, 0.96963645, 1.        ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_random_cov_matrix(n=4, var_range=(0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target rho \n",
      "[[1.         0.23341041 0.25925644]\n",
      " [0.23341041 1.         0.24453444]\n",
      " [0.25925644 0.24453444 1.        ]]\n",
      "achieved rho \n",
      "          0         1         2\n",
      "0  1.000000  0.233385  0.300786\n",
      "1  0.233385  1.000000  0.233493\n",
      "2  0.300786  0.233493  1.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.649692</td>\n",
       "      <td>0.282440</td>\n",
       "      <td>0.372321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.586274</td>\n",
       "      <td>0.581331</td>\n",
       "      <td>0.667341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.921669</td>\n",
       "      <td>-0.149230</td>\n",
       "      <td>-0.736351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.622843</td>\n",
       "      <td>0.583204</td>\n",
       "      <td>2.504634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.472534</td>\n",
       "      <td>0.816746</td>\n",
       "      <td>-1.320984</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          y        x1        x2\n",
       "0 -1.649692  0.282440  0.372321\n",
       "1 -0.586274  0.581331  0.667341\n",
       "2 -0.921669 -0.149230 -0.736351\n",
       "3  1.622843  0.583204  2.504634\n",
       "4 -1.472534  0.816746 -1.320984"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmat = make_random_cov_matrix(3, var_range=(0,.4))\n",
    "\n",
    "cdata = generate_corr_data_multivariate(rmat, 1000, seed=1234, colnames=['y','x1','x2'])\n",
    "cdata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate groupwise correlated data with binary dependent variable\n",
    "\n",
    "This is what we're really going need, since almost all cases of fairness testing are against categorical variables -- even correlation with age is against age brackets rather than continuous age.\n",
    "\n",
    "This gets tricky, since we have to mix binomial and multivariate normal distributions. I'm going to cheat, slightly, by converting a multivariate normal distribution to a binary distribution. This has the benefit of approximating the true nature of most classification (as they have some unknown latent continuity under their labels), but has the consequence of only being proportional to our input covariance matrix (see introduction section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import percentileofscore\n",
    "\n",
    "def generate_group_data(n_variables, \n",
    "                        groups, \n",
    "                        var_cov, \n",
    "                        group_ES=None, \n",
    "                        es_type='pct',\n",
    "                        target_name='y', \n",
    "                        size=10000,\n",
    "                        random_seed=None):\n",
    "        \"\"\"\n",
    "        Master function for producing randomly generated datasets with groupwise differences\n",
    "        for use in testing various fairness systems.\n",
    "        \n",
    "        Note that this function will return values of only approximate group differences \n",
    "        and covariance. \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        n_varibles : int\n",
    "            number of independent variables to generate\n",
    "            creates one vector for each IV\n",
    "        groups : list(str)\n",
    "            list of group names to divide data\n",
    "            if only one group is provided, generates normally distributed data\n",
    "        var_cov : np.ndarray or str\n",
    "            covariance matrix for the desired multivariate normal distribution\n",
    "            alternatively, can input string commans to generate covariances within range\n",
    "            'low' : correlations between (0,.2)\n",
    "            'med' : correlations between (.2,.6)\n",
    "            'high' : correlations between (.6,1)\n",
    "        group_ES : list\n",
    "            effect size metrics for each group, relative to population mean\n",
    "            see es_type for a description of values and expected behavior\n",
    "        es_type : str\n",
    "            type of effect size metrics entered in group_ES\n",
    "            'pct' and 'prop' are most useful for segmenting groups for classification\n",
    "            'd' is best used when segmenting groups on continuous metrics\n",
    "            \n",
    "            'pct' : percentile pass rates of each group (0, 100)\n",
    "            'prop': proportional pass rates of each group (0,1)\n",
    "            'd'   : effect size, d, difference between group and population mean\n",
    "        target_name : str\n",
    "            name for dependent variable, aka predictor\n",
    "            defaults to 'y' by convention\n",
    "        size : int\n",
    "            number of rows of data to generate\n",
    "        random_seed : int\n",
    "            seed for random state\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        generated_df : pandas.DataFrame\n",
    "            dataframe of generated data with covariance, var_cov, \n",
    "            and group means segmented by group_ES\n",
    "        \"\"\"\n",
    "        var_names = ['x'+str(i) for i in range(1,n_variables+1)]\n",
    "        colnames = [target_name] + var_names\n",
    "\n",
    "        if random_seed:\n",
    "            np.random.seed(random_seed)\n",
    "\n",
    "        mu = np.zeros(len(colnames))\n",
    "\n",
    "        if isinstance(var_cov, str):\n",
    "            #generate covariance matrix \n",
    "            print('generating random covariance matrix\\n')\n",
    "            if var_cov == 'low':\n",
    "                var_cov = make_random_cov_matrix(n=len(colnames), var_range=(0,.2))\n",
    "            elif var_cov == 'med':\n",
    "                var_cov = make_random_cov_matrix(n=len(colnames), var_range=(.2,.6))\n",
    "            elif var_cov == 'high':\n",
    "                var_cov = make_random_cov_matrix(n=len(colnames), var_range=(.6,1))\n",
    "\n",
    "        mat = np.random.multivariate_normal(mu, cov=var_cov,size=size)\n",
    "\n",
    "        output = pd.DataFrame(mat, columns=colnames)\n",
    "        output['group'] = None\n",
    "\n",
    "        batch_size = np.ceil(len(output)/len(groups))\n",
    "\n",
    "        for b, chunk in output.groupby(np.arange(len(output)) // batch_size):\n",
    "            b = int(b) #really weird numpy behavior leads to \"-0\"\n",
    "            output.loc[chunk.index, 'group'] = groups[b]\n",
    "            #calculate noncentrality parameter from type of effect size\n",
    "            if es_type=='prop':\n",
    "                # calculate from proprortional difference from 0\n",
    "                prop = group_ES[b]\n",
    "                # restrict to (0,100)\n",
    "                pct = max(max(50 + prop*100, 100), 0)\n",
    "            elif es_type == 'pct':\n",
    "                # calculate from percentile\n",
    "                pct = group_ES[b]\n",
    "            else:\n",
    "                # calculate from effect size d\n",
    "                d = group_ES[b]\n",
    "                # empirical percentile of score\n",
    "                pct = percentileofscore(chunk.y, chunk.y.mean()+d)\n",
    "            pct = max(min(pct, 100), 0) #restrict percentile range\n",
    "            ncp = np.percentile(chunk.y, pct) - chunk.y.mean() \n",
    "\n",
    "            output.loc[chunk.index, 'y'] = chunk['y'] + ncp\n",
    "\n",
    "        output[target_name+'_b'] = np.array(output[target_name] > 0, dtype=int)\n",
    "        output = output.join(pd.get_dummies(output.group,prefix='group', drop_first=False))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating random covariance matrix\n",
      "\n",
      "group\n",
      "F    0.4928\n",
      "M    0.6594\n",
      "Name: y_b, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group_M</th>\n",
       "      <th>group_F</th>\n",
       "      <th>y_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>group_M</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.168564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_F</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.168564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_b</th>\n",
       "      <td>0.168564</td>\n",
       "      <td>-0.168564</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          group_M   group_F       y_b\n",
       "group_M  1.000000 -1.000000  0.168564\n",
       "group_F -1.000000  1.000000 -0.168564\n",
       "y_b      0.168564 -0.168564  1.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups=['M','F']\n",
    "data = generate_group_data(groups=groups, \n",
    "                           n_variables=4, \n",
    "                           var_cov='low', \n",
    "                           group_ES=[65,50], \n",
    "                           es_type='pct',\n",
    "                           target_name='y', \n",
    "                           size=10000)\n",
    "print(data.groupby('group')['y_b'].mean())\n",
    "\n",
    "gnames = ['group_'+g for g in groups]\n",
    "gnames.append('y_b')\n",
    "\n",
    "data.corr().loc[gnames, gnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating random covariance matrix\n",
      "\n",
      "group\n",
      "Asian              0.001895\n",
      "Black             -0.106181\n",
      "Hispanic/Latino   -0.162379\n",
      "White              0.109364\n",
      "Name: y, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group_Asian</th>\n",
       "      <th>group_Black</th>\n",
       "      <th>group_Hispanic/Latino</th>\n",
       "      <th>group_White</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>group_Asian</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>0.023623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_Black</th>\n",
       "      <td>-0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.038314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_Hispanic/Latino</th>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.070521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_White</th>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>-0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.085212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>0.023623</td>\n",
       "      <td>-0.038314</td>\n",
       "      <td>-0.070521</td>\n",
       "      <td>0.085212</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       group_Asian  group_Black  group_Hispanic/Latino  \\\n",
       "group_Asian               1.000000    -0.333333              -0.333333   \n",
       "group_Black              -0.333333     1.000000              -0.333333   \n",
       "group_Hispanic/Latino    -0.333333    -0.333333               1.000000   \n",
       "group_White              -0.333333    -0.333333              -0.333333   \n",
       "y                         0.023623    -0.038314              -0.070521   \n",
       "\n",
       "                       group_White         y  \n",
       "group_Asian              -0.333333  0.023623  \n",
       "group_Black              -0.333333 -0.038314  \n",
       "group_Hispanic/Latino    -0.333333 -0.070521  \n",
       "group_White               1.000000  0.085212  \n",
       "y                         0.085212  1.000000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups=['Asian','Black','Hispanic/Latino','White']\n",
    "data = generate_group_data(groups=groups, \n",
    "                           n_variables=4, \n",
    "                           var_cov='low', \n",
    "                           group_ES=[0,-.1,-.15,.1], \n",
    "                           es_type='d',\n",
    "                           target_name='y', \n",
    "                           size=10000)\n",
    "print(data.groupby('group')['y'].mean())\n",
    "\n",
    "gnames = ['group_'+g for g in groups]\n",
    "gnames.append('y')\n",
    "\n",
    "data.corr().loc[gnames, gnames]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demonstrating use of imports\n",
    "\n",
    "Replicating the above, but with imports rather than inline functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairML.data import data_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating data with approxmate rho=0.5\n",
      "\n",
      "seeded X mean:  0.019, sd:  0.979\n",
      "seeded y mean:  0.071, sd:  0.997\n",
      "simulated y (z) mean:  0.082, sd:  1.126\n",
      "\n",
      "adjusted X mean:  0.019, sd:  0.979\n",
      "adjusted simulated y (z) mean:  0.082, sd:  1.126\n",
      "\n",
      "correlation of z~X: 0.466\n"
     ]
    }
   ],
   "source": [
    "z, X = data_generator.generate_corr_data_univariate(rho= .5, N = 1000, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target rho \n",
      "[[1.         0.1628426  0.02640394]\n",
      " [0.1628426  1.         0.13952821]\n",
      " [0.02640394 0.13952821 1.        ]]\n",
      "achieved rho \n",
      "          0         1         2\n",
      "0  1.000000  0.177243  0.040657\n",
      "1  0.177243  1.000000  0.152110\n",
      "2  0.040657  0.152110  1.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.368200</td>\n",
       "      <td>-1.312755</td>\n",
       "      <td>1.197407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.148249</td>\n",
       "      <td>-0.353520</td>\n",
       "      <td>1.052132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.919647</td>\n",
       "      <td>-0.675096</td>\n",
       "      <td>0.024231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.562133</td>\n",
       "      <td>1.079660</td>\n",
       "      <td>0.720584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-2.028659</td>\n",
       "      <td>-0.545323</td>\n",
       "      <td>0.878137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          y        x1        x2\n",
       "0 -0.368200 -1.312755  1.197407\n",
       "1  0.148249 -0.353520  1.052132\n",
       "2 -0.919647 -0.675096  0.024231\n",
       "3  2.562133  1.079660  0.720584\n",
       "4 -2.028659 -0.545323  0.878137"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmat = data_generator.make_random_cov_matrix(3, var_range=(0,.4))\n",
    "\n",
    "cdata = data_generator.generate_corr_data_multivariate(rmat, 1000, seed=1234, colnames=['y','x1','x2'])\n",
    "cdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating random covariance matrix\n",
      "\n",
      "group\n",
      "F    0.4956\n",
      "M    0.6564\n",
      "Name: y_b, dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>group_M</th>\n",
       "      <th>group_F</th>\n",
       "      <th>y_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>group_M</th>\n",
       "      <td>1.00000</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>0.16269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_F</th>\n",
       "      <td>-1.00000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-0.16269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_b</th>\n",
       "      <td>0.16269</td>\n",
       "      <td>-0.16269</td>\n",
       "      <td>1.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         group_M  group_F      y_b\n",
       "group_M  1.00000 -1.00000  0.16269\n",
       "group_F -1.00000  1.00000 -0.16269\n",
       "y_b      0.16269 -0.16269  1.00000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groups=['M','F']\n",
    "data = data_generator.generate_group_data(groups=groups, \n",
    "                           n_variables=4, \n",
    "                           var_cov='low', \n",
    "                           group_ES=[65,50], \n",
    "                           es_type='pct',\n",
    "                           target_name='y', \n",
    "                           size=10000)\n",
    "print(data.groupby('group')['y_b'].mean())\n",
    "\n",
    "gnames = ['group_'+g for g in groups]\n",
    "gnames.append('y_b')\n",
    "\n",
    "data.corr().loc[gnames, gnames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lbdev]",
   "language": "python",
   "name": "conda-env-lbdev-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
